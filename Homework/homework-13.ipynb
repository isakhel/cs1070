{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import List\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.\n",
    "A neural network is a machine learning model that tries to mimic a brain by \"thinking\". It assigns weights to variables and makes changes to the weights by running the model multiple times and getting error feedback. A NN has 3 main parts: Input Layer, Output Layer, and hidden layers where the thinking occurs. Building a NN requires 2 main things, the function that changes the variables at each node to do the thinking (activation function) and the loss function that tells the machine by how much its prediction was off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.\n",
    "A loss function is the simplest way to check the performance of a NN because it shows the prediction error. Another good way to check is to use k-fold cross validation, because it lets every part of the data be tested at least once and runs multiple tests instead of just one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Credit_Card.csv')\n",
    "label_df = pd.read_csv('Credit_card_label.csv')\n",
    "sorted_df = pd.merge(df, label_df, on='Ind_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datacleaner(sorted_df: pd.DataFrame,\n",
    "                        numerical_cols: List[str] = ['Annual_income', 'Birthday_count'], \n",
    "                          categorical_cols: List[str] = ['GENDER', 'Car_Owner', 'Propert_Owner', 'Type_Income', \n",
    "                                                         'EDUCATION', 'Marital_status', 'Housing_type', 'Type_Occupation'], \n",
    "                          occupation_col: str = 'Type_Occupation') -> pd.DataFrame:\n",
    "    for col in numerical_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    sorted_df.dropna(subset=[occupation_col], inplace=True)\n",
    "    \n",
    "    sorted_df['GENDER'] = sorted_df['GENDER'].fillna(sorted_df['GENDER'].mode()[0])\n",
    "    \n",
    "    sorted_df = pd.get_dummies(sorted_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    sorted_df = sorted_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    sorted_df = sorted_df.astype(int)\n",
    "    scaler = StandardScaler()\n",
    "    sorted_df[numerical_cols] = scaler.fit_transform(sorted_df[numerical_cols])\n",
    "    return sorted_df\n",
    "sorted_df = datacleaner(sorted_df)\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\n",
    "I normalized the numerical columns and tweaked the filling null values in addition to the data cleaning that was already done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self,input_features=8,\n",
    "                 hidden1=20,hidden2=20,\n",
    "                 out_features=2):\n",
    "        super().__init__() \n",
    "        \"\"\"\n",
    "        super is a computed indirect reference\n",
    "        which means that it isolates changes and\n",
    "        makes sure the children in the layers of\n",
    "        multiple inheritance are calling \n",
    "        the right parents\n",
    "        \"\"\"\n",
    "        self.layer_1_connection = nn.Linear(input_features, hidden1)\n",
    "        self.layer_2_connection = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #apply activation function\n",
    "        x = F.relu(self.layer_1_connection(x))\n",
    "        x = F.relu(self.layer_2_connection(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# create an instance of the model\n",
    "ann = ANN_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch ANN Test Accuracy: 0.8537735849056604\n"
     ]
    }
   ],
   "source": [
    "X = sorted_df.drop(columns=['label']).values \n",
    "y = sorted_df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long) \n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "input_features = X_train.shape[1]\n",
    "model = ANN_Model(input_features=input_features, hidden1=20, hidden2=20, out_features=2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch) \n",
    "        loss = criterion(outputs, y_batch) \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred_class = torch.argmax(y_pred, dim=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred_class.numpy())\n",
    "    print(f\"PyTorch ANN Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_regression_overfit(ycol) -> float:\n",
    "    X = sorted_df.drop(columns=[ycol])\n",
    "    y = sorted_df[ycol]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_standard = scaler.fit_transform(X_train)\n",
    "    X_test_standard = scaler.transform(X_test)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_train_standard, y_train)\n",
    "    \n",
    "    log_reg = LogisticRegression(max_iter=100)\n",
    "    log_reg.fit(X_resampled, y_resampled)\n",
    "\n",
    "    y_pred = log_reg.predict(X_test_standard)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6415094339622641\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.70      0.77       181\n",
      "           1       0.14      0.29      0.19        31\n",
      "\n",
      "    accuracy                           0.64       212\n",
      "   macro avg       0.50      0.50      0.48       212\n",
      "weighted avg       0.75      0.64      0.69       212\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = \"label\"\n",
    "accuracy, report = Logistic_regression_overfit(input)\n",
    "print(accuracy)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.\n",
    "The neural networks performed better than the logisic regression even when it was oversampled. The reason is probably this dataset had a very difficult patterns system that a simple model like logistic regression wasn't able to work with properly, while due to the nature of a NN it can handle more complex tasks. Of course there is a small danger of overfitting so we probably shouldn't forget that either."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
